"""Streaming generator for copy generation."""
from __future__ import annotations

import asyncio
import json
import logging
import time
from typing import AsyncGenerator

from app.models.product import Product
from app.schemas.copy_schemas import CopyStyle
from app.services.llm_client import get_llm_client, LLMClientError

logger = logging.getLogger(__name__)


class StreamingGenerator:
    """Generator for streaming copy content."""

    @staticmethod
    async def generate_copy_stream(
        product: Product,
        style: CopyStyle = CopyStyle.natural,
    ) -> AsyncGenerator[str, None]:
        """
        Generate streaming copy for WeChat Moments posts using real LLM.
        
        First chunk is emitted within 500ms to meet latency requirement.
        Falls back to template-based generation if LLM fails.
        
        Args:
            product: Product instance
            style: Copy style (natural, professional, funny)
            
        Yields:
            JSON-encoded chunks containing post content in SSE format
        """
        gen_start_time = time.time()
        
        logger.info(f"[GENERATOR] ========== Streaming Generator Started ==========")
        logger.info(f"[GENERATOR] Input: product_name={product.name}, style={style.value}, tags={product.tags}")
        
        # Extract product information
        product_name = product.name
        tags = product.tags or []
        tags_str = "ã€".join(tags) if tags else "æ—¶å°š"
        attributes = product.attributes or {}
        color = attributes.get("color", "")
        scene = attributes.get("scene", "")
        
        logger.info(f"[GENERATOR] Extracted info: name={product_name}, tags_str={tags_str}, color={color}, scene={scene}")
        
        # Send initial chunk immediately (within 500ms requirement)
        logger.info(f"[GENERATOR] Step 1: Sending initial chunk...")
        initial_chunk = {
            "type": "start",
            "total": 3,  # Always generate 3 posts
            "style": style.value,
        }
        first_chunk_time = time.time() - gen_start_time
        logger.info(f"[GENERATOR] âœ“ First chunk sent in {first_chunk_time*1000:.2f}ms (requirement: <500ms)")
        yield f"data: {json.dumps(initial_chunk, ensure_ascii=False)}\n\n"
        
        # Try to use real LLM, fallback to templates if it fails
        use_llm = True
        llm_client = get_llm_client()
        
        # Check if LLM is available
        if not llm_client.settings.llm_api_key or not llm_client.settings.llm_base_url:
            logger.warning("[GENERATOR] LLM credentials not available, using template fallback")
            use_llm = False
        
        # Generate 3 posts
        posts = []
        style_prompts = {
            CopyStyle.natural: "è‡ªç„¶ã€äº²åˆ‡ã€æ—¥å¸¸",
            CopyStyle.professional: "ä¸“ä¸šã€æƒå¨ã€å¯ä¿¡",
            CopyStyle.funny: "å¹½é»˜ã€æœ‰è¶£ã€è½»æ¾",
        }
        style_desc = style_prompts.get(style, "è‡ªç„¶")
        
        for post_idx in range(1, 4):
            logger.info(f"[GENERATOR] Step 2.{post_idx}: Generating post {post_idx}/3...")
            
            # Send post start
            chunk = {
                "type": "post_start",
                "index": post_idx,
                "total": 3,
            }
            yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.01)
            
            post_content = ""
            post_start_time = time.time()
            
            if use_llm:
                # Build prompt for LLM
                prompt = f"""è¯·ä¸ºä»¥ä¸‹å•†å“å†™ä¸€æ¡æœ‹å‹åœˆæ–‡æ¡ˆï¼ˆç¬¬{post_idx}æ¡ï¼Œå…±3æ¡ï¼‰ï¼š

å•†å“åç§°ï¼š{product_name}
å•†å“æ ‡ç­¾ï¼š{tags_str}
é¢œè‰²ï¼š{color if color else "ç»å…¸è‰²"}
é€‚ç”¨åœºæ™¯ï¼š{scene if scene else "å¤šåœºæ™¯"}

è¦æ±‚ï¼š
1. é£æ ¼ï¼š{style_desc}
2. é•¿åº¦ï¼š30-50å­—
3. è¦æœ‰å¸å¼•åŠ›ï¼Œèƒ½å¼•èµ·è´­ä¹°æ¬²æœ›
4. ä¸è¦é‡å¤ä¹‹å‰çš„å†…å®¹
5. è¯­è¨€è‡ªç„¶æµç•…ï¼Œç¬¦åˆæœ‹å‹åœˆé£æ ¼

åªè¾“å‡ºæ–‡æ¡ˆå†…å®¹ï¼Œä¸è¦å…¶ä»–è¯´æ˜ï¼š"""
                
                system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‹ç±»é”€å”®æ–‡æ¡ˆå†™æ‰‹ï¼Œæ“…é•¿å†™å¸å¼•äººçš„æœ‹å‹åœˆæ–‡æ¡ˆã€‚"
                
                try:
                    logger.info(f"[GENERATOR] Calling LLM for post {post_idx}...")
                    # Stream from LLM
                    async for llm_chunk in llm_client.stream_chat(
                        prompt,
                        system=system_prompt,
                        temperature=0.8,
                        max_tokens=150,
                    ):
                        if llm_chunk:
                            post_content += llm_chunk
                            # Stream token chunk
                            token_chunk = {
                                "type": "token",
                                "content": llm_chunk,
                                "index": post_idx,
                                "position": len(post_content) - len(llm_chunk),
                            }
                            yield f"data: {json.dumps(token_chunk, ensure_ascii=False)}\n\n"
                    
                    post_content = post_content.strip()
                    llm_time = (time.time() - post_start_time) * 1000
                    logger.info(f"[GENERATOR] âœ“ Post {post_idx} generated by LLM ({len(post_content)} chars, {llm_time:.1f}ms)")
                    
                except LLMClientError as e:
                    logger.warning(f"[GENERATOR] LLM failed for post {post_idx}: {e}, using template fallback")
                    use_llm = False  # Switch to templates for remaining posts
                except Exception as e:
                    logger.error(f"[GENERATOR] Unexpected error in LLM for post {post_idx}: {e}", exc_info=True)
                    use_llm = False
            
            # Fallback to template if LLM not used or failed
            if not post_content:
                logger.info(f"[GENERATOR] Using template for post {post_idx}")
                post_content = StreamingGenerator._generate_template_post(
                    product_name, tags_str, style, post_idx
                )
                # Stream template content
                chunk_size = 5
                for i in range(0, len(post_content), chunk_size):
                    chunk_text = post_content[i:i + chunk_size]
                    token_chunk = {
                        "type": "token",
                        "content": chunk_text,
                        "index": post_idx,
                        "position": i,
                    }
                    yield f"data: {json.dumps(token_chunk, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.01)
            
            # Send post end
            chunk = {
                "type": "post_end",
                "index": post_idx,
                "content": post_content,
            }
            posts.append(post_content)
            logger.info(f"[GENERATOR] âœ“ Post {post_idx} completed: {post_content[:50]}...")
            yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.02)
        
        # Send completion
        logger.info(f"[GENERATOR] Step 3: Sending completion chunk...")
        chunk = {
            "type": "complete",
            "posts": posts,
        }
        total_time = time.time() - gen_start_time
        logger.info(f"[GENERATOR] âœ“ All posts streamed. Total time: {total_time*1000:.2f}ms")
        logger.info(f"[GENERATOR] ========== Streaming Generator Completed ==========")
        yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
    
    @staticmethod
    def _generate_template_post(
        product_name: str,
        tags_str: str,
        style: CopyStyle,
        post_index: int,
    ) -> str:
        """Generate a post from template (fallback method)."""
        style_templates = {
            CopyStyle.natural: [
                "ä»Šå¤©æ¨èè¿™æ¬¾{name}ï¼Œ{tags}çš„è®¾è®¡çœŸçš„å¾ˆèµï¼é€‚åˆæ—¥å¸¸ç©¿æ­ï¼Œå¿«æ¥ç§ä¿¡æˆ‘äº†è§£æ›´å¤šï½",
                "åˆšå…¥æ‰‹äº†{name}ï¼Œ{tags}çš„æ­é…å¤ªé€‚åˆæ—¥å¸¸äº†ï½è½»æ¾ç©¿å‡ºå¥½æ°”è´¨ï¼Œå¿ƒåŠ¨ä¸å¦‚è¡ŒåŠ¨ï¼",
                "åˆ†äº«ä¸€ä¸ªè¶…å¥½ç©¿çš„{name}ï¼Œ{tags}é£æ ¼ï¼Œæ¨èç»™å¤§å®¶ï¼æ— è®ºæ˜¯é€šå‹¤è¿˜æ˜¯é€›è¡—éƒ½è¶…é€‚åˆï½",
            ],
            CopyStyle.professional: [
                "ã€æ–°å“æ¨èã€‘{name}ï¼Œé‡‡ç”¨{tags}å·¥è‰ºï¼Œå“è´¨å“è¶Šï¼Œå€¼å¾—æ‹¥æœ‰ã€‚ä¸“ä¸šè®¤è¯ï¼Œå“è´¨ä¿è¯ã€‚",
                "ä¸“ä¸šæ¨èï¼š{name}ï¼Œ{tags}ç‰¹æ€§çªå‡ºï¼Œé€‚åˆè¿½æ±‚å“è´¨çš„ä½ ã€‚ç‚¹å‡»é“¾æ¥æŸ¥çœ‹è¯¦æƒ…ã€‚",
                "ç²¾é€‰å¥½ç‰©ï¼š{name}ï¼Œ{tags}è®¾è®¡ï¼Œä¸“ä¸šè®¤è¯ï¼Œå“è´¨ä¿è¯ã€‚é™æ—¶ä¼˜æƒ ï¼Œä¸å®¹é”™è¿‡ã€‚",
            ],
            CopyStyle.funny: [
                "å“ˆå“ˆå“ˆï¼Œè¿™åŒ{name}å¤ªå¯çˆ±äº†ï¼{tags}çš„è®¾è®¡è®©æˆ‘å¿ä¸ä½æƒ³ç¬‘ï½ç©¿ä¸Šå®ƒå¿ƒæƒ…éƒ½å˜å¥½äº†ğŸ˜„",
                "ç©¿ä¸Š{name}æ„Ÿè§‰è‡ªå·±å¹´è½»äº†10å²ï¼{tags}é£æ ¼å¤ªæœ‰è¶£äº†ï¼Œæœ‹å‹ä»¬éƒ½è¯´å¥½çœ‹ï½",
                "è¿™åŒ{name}ç®€ç›´æ˜¯å¿«ä¹æºæ³‰ï¼{tags}çš„æ­é…è®©äººå¿ƒæƒ…éƒ½å˜å¥½äº†ï½å¿«æ¥ä¸€èµ·å¼€å¿ƒå§ï¼",
            ],
        }
        
        templates = style_templates.get(style, style_templates[CopyStyle.natural])
        template_idx = (post_index - 1) % len(templates)
        return templates[template_idx].format(name=product_name, tags=tags_str)

