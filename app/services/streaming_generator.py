"""Streaming generator for copy generation."""
from __future__ import annotations

import asyncio
import json
import logging
import time
from typing import AsyncGenerator

from app.models.product import Product
from app.schemas.copy_schemas import CopyStyle

logger = logging.getLogger(__name__)


class StreamingGenerator:
    """Generator for streaming copy content."""

    @staticmethod
    async def generate_copy_stream(
        product: Product,
        style: CopyStyle = CopyStyle.natural,
    ) -> AsyncGenerator[str, None]:
        """
        Generate streaming copy for WeChat Moments posts.
        
        First chunk is emitted within 500ms to meet latency requirement.
        
        Args:
            product: Product instance
            style: Copy style (natural, professional, funny)
            
        Yields:
            JSON-encoded chunks containing post content in SSE format
        """
        gen_start_time = time.time()
        
        logger.info(f"[GENERATOR] ========== Streaming Generator Started ==========")
        logger.info(f"[GENERATOR] Input: product_name={product.name}, style={style.value}, tags={product.tags}")
        
        # Extract product information
        product_name = product.name
        tags = product.tags or []
        tags_str = "ã€".join(tags) if tags else "æ—¶å°š"
        logger.info(f"[GENERATOR] Extracted info: name={product_name}, tags_str={tags_str}")
        
        # Style-specific templates
        style_templates = {
            CopyStyle.natural: [
                "ä»Šå¤©æ¨èè¿™æ¬¾{name}ï¼Œ{tags}çš„è®¾è®¡çœŸçš„å¾ˆèµï¼é€‚åˆæ—¥å¸¸ç©¿æ­ï¼Œå¿«æ¥ç§ä¿¡æˆ‘äº†è§£æ›´å¤šï½",
                "åˆšå…¥æ‰‹äº†{name}ï¼Œ{tags}çš„æ­é…å¤ªé€‚åˆæ—¥å¸¸äº†ï½è½»æ¾ç©¿å‡ºå¥½æ°”è´¨ï¼Œå¿ƒåŠ¨ä¸å¦‚è¡ŒåŠ¨ï¼",
                "åˆ†äº«ä¸€ä¸ªè¶…å¥½ç©¿çš„{name}ï¼Œ{tags}é£æ ¼ï¼Œæ¨èç»™å¤§å®¶ï¼æ— è®ºæ˜¯é€šå‹¤è¿˜æ˜¯é€›è¡—éƒ½è¶…é€‚åˆï½",
            ],
            CopyStyle.professional: [
                "ã€æ–°å“æ¨èã€‘{name}ï¼Œé‡‡ç”¨{tags}å·¥è‰ºï¼Œå“è´¨å“è¶Šï¼Œå€¼å¾—æ‹¥æœ‰ã€‚ä¸“ä¸šè®¤è¯ï¼Œå“è´¨ä¿è¯ã€‚",
                "ä¸“ä¸šæ¨èï¼š{name}ï¼Œ{tags}ç‰¹æ€§çªå‡ºï¼Œé€‚åˆè¿½æ±‚å“è´¨çš„ä½ ã€‚ç‚¹å‡»é“¾æ¥æŸ¥çœ‹è¯¦æƒ…ã€‚",
                "ç²¾é€‰å¥½ç‰©ï¼š{name}ï¼Œ{tags}è®¾è®¡ï¼Œä¸“ä¸šè®¤è¯ï¼Œå“è´¨ä¿è¯ã€‚é™æ—¶ä¼˜æƒ ï¼Œä¸å®¹é”™è¿‡ã€‚",
            ],
            CopyStyle.funny: [
                "å“ˆå“ˆå“ˆï¼Œè¿™åŒ{name}å¤ªå¯çˆ±äº†ï¼{tags}çš„è®¾è®¡è®©æˆ‘å¿ä¸ä½æƒ³ç¬‘ï½ç©¿ä¸Šå®ƒå¿ƒæƒ…éƒ½å˜å¥½äº†ğŸ˜„",
                "ç©¿ä¸Š{name}æ„Ÿè§‰è‡ªå·±å¹´è½»äº†10å²ï¼{tags}é£æ ¼å¤ªæœ‰è¶£äº†ï¼Œæœ‹å‹ä»¬éƒ½è¯´å¥½çœ‹ï½",
                "è¿™åŒ{name}ç®€ç›´æ˜¯å¿«ä¹æºæ³‰ï¼{tags}çš„æ­é…è®©äººå¿ƒæƒ…éƒ½å˜å¥½äº†ï½å¿«æ¥ä¸€èµ·å¼€å¿ƒå§ï¼",
            ],
        }
        
        templates = style_templates.get(style, style_templates[CopyStyle.natural])
        logger.info(f"[GENERATOR] Selected {len(templates)} templates for style: {style.value}")
        
        # Generate 3 posts immediately (no delay for first chunk)
        logger.info(f"[GENERATOR] Step 1: Generating posts from templates...")
        posts = []
        for i, template in enumerate(templates[:3]):
            post = template.format(name=product_name, tags=tags_str)
            posts.append(post)
            logger.info(f"[GENERATOR]   Post {i+1} generated: {post[:50]}...")
        logger.info(f"[GENERATOR] âœ“ Generated {len(posts)} posts")
        
        # Send initial chunk immediately (within 500ms requirement)
        logger.info(f"[GENERATOR] Step 2: Sending initial chunk...")
        initial_chunk = {
            "type": "start",
            "total": len(posts),
            "style": style.value,
        }
        first_chunk_time = time.time() - gen_start_time
        logger.info(f"[GENERATOR] âœ“ First chunk sent in {first_chunk_time*1000:.2f}ms (requirement: <500ms)")
        logger.info(f"[GENERATOR] Initial chunk data: {json.dumps(initial_chunk, ensure_ascii=False)}")
        yield f"data: {json.dumps(initial_chunk, ensure_ascii=False)}\n\n"
        
        # Stream posts one by one with small delays
        logger.info(f"[GENERATOR] Step 3: Streaming posts...")
        for idx, post in enumerate(posts):
            logger.info(f"[GENERATOR]   Streaming post {idx + 1}/{len(posts)}...")
            # Send post start
            chunk = {
                "type": "post_start",
                "index": idx + 1,
                "total": len(posts),
            }
            logger.debug(f"[GENERATOR]     Sending post_start chunk for post {idx + 1}")
            yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.02)  # Small delay for streaming effect
            
            # Stream post content in chunks (not character by character for better performance)
            chunk_size = 5  # Stream 5 characters at a time
            token_count = 0
            for i in range(0, len(post), chunk_size):
                chunk_text = post[i:i + chunk_size]
                chunk = {
                    "type": "token",
                    "content": chunk_text,
                    "index": idx + 1,
                    "position": i,
                }
                token_count += 1
                if token_count <= 3:  # Log first few tokens
                    logger.debug(f"[GENERATOR]     Token {token_count}: {chunk_text}")
                yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.01)  # Small delay between chunks
            
            # Send post end
            chunk = {
                "type": "post_end",
                "index": idx + 1,
                "content": post,
            }
            logger.info(f"[GENERATOR]     âœ“ Post {idx + 1} completed ({len(post)} chars, {token_count} tokens)")
            yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.02)
        
        # Send completion
        logger.info(f"[GENERATOR] Step 4: Sending completion chunk...")
        chunk = {
            "type": "complete",
            "posts": posts,
        }
        total_time = time.time() - gen_start_time
        logger.info(f"[GENERATOR] âœ“ All posts streamed. Total time: {total_time*1000:.2f}ms")
        logger.info(f"[GENERATOR] Completion data: {json.dumps(chunk, ensure_ascii=False)}")
        logger.info(f"[GENERATOR] ========== Streaming Generator Completed ==========")
        yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

